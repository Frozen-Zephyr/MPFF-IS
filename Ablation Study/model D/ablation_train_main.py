import time
import optuna
import random
import numpy as np
import torch
import os

from torch.nn import BCEWithLogitsLoss
from sklearn.metrics import f1_score, matthews_corrcoef
from torch.utils.data import DataLoader
from torch.utils.tensorboard import  SummaryWriter
from torch.utils.data import RandomSampler
from ablation_mpnn_predictor import MPNNPredictorWithProtein
from ablation_loading_data import MolecularDataset
import dgl
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc,precision_score, recall_score, confusion_matrix



def set_seed(seed=42):
    """å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒç»“æœä¸€è‡´"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    dgl.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)





def early_stopping(avg_valid_loss,
                   avg_valid_acc,
                   best_valid_loss,
                   model,
                   epoch,
                   epochs_without_improvement,
                   best_model_state,
                   best_valid_acc,
                   patience=10):

    if best_valid_acc < avg_valid_acc:
        best_valid_acc = avg_valid_acc
        best_valid_loss = avg_valid_loss
        best_model_state = model.state_dict()
        epochs_without_improvement = 0
        return 'continue', best_valid_loss, epochs_without_improvement, best_model_state, best_valid_acc
    else:
        if avg_valid_loss < best_valid_loss:
            # best_valid_loss = avg_valid_loss
            epochs_without_improvement = 0
            # best_model_state = model.state_dict()  # Save the best model
            # torch.save(model.state_dict(), 'best_model{}.pth'.format(epoch + 1))  # Save model weights
            return 'continue', best_valid_loss, epochs_without_improvement, best_model_state,best_valid_acc
        else:
            epochs_without_improvement += 1
            if epochs_without_improvement >= patience:
                print(f"Early stopping triggered at epoch {epoch + 1}")
                return 'end', best_valid_loss, epochs_without_improvement, best_model_state,best_valid_acc
            else:
                return 'continue', best_valid_loss, epochs_without_improvement, best_model_state,best_valid_acc


def train_valid(model,
                loss_trans,
                learning_rate,
                weight_decay,
                batch_size,
                epochs,
                device,
                dataset_train,
                dataset_valid,
                writer,
                trial_id=None,
                seed=None,
                trial_number='default'
                ):

    if trial_id is None:
        if seed is not None:
            set_seed(seed)
        elif seed is None:
            seed = 42
            set_seed(seed)

    elif trial_id is not None:
        seed = trial_id
        set_seed(seed)



    # writer = SummaryWriter('train_logs')
    time_start = time.time()
    generator = torch.Generator()
    generator.manual_seed(seed)

    # ğŸ¯ ä½¿ç”¨ RandomSampler ç¡®ä¿æ¯ä¸ª trial æ•°æ®é¡ºåºä¸åŒ



    dataloader_train = DataLoader(dataset_train,batch_size=batch_size, collate_fn=MolecularDataset.collate_fn,
                                  sampler=RandomSampler(dataset_train, generator=generator),

                                  num_workers=0,  pin_memory=False)# æŒ‡å®šè‡ªå®šä¹‰çš„ collate_fn
    dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size,collate_fn=MolecularDataset.collate_fn,
                                  num_workers=0,  pin_memory=False)
    # dataloader_kras_test = DataLoader(kras_test, batch_size=batch_size, collate_fn=MolecularDataset.collate_fn,
    #                                 num_workers=0, pin_memory=False)

    optim = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)      #ä¼˜åŒ–å™¨é€‰æ‹©ï¼šAdamW

    best_valid_loss = float('inf')  # åˆå§‹åŒ–æœ€ä½³éªŒè¯æŸå¤±
    best_valid_acc= float('-inf')
    epochs_without_improvement = 0  # è®¡æ•°å™¨ï¼Œè®°å½•å¤šå°‘è½®æ²¡æœ‰æå‡
    best_model_state = None  # ç”¨äºä¿å­˜æœ€å¥½çš„æ¨¡å‹

    epoch_rounds=0
    train_round=0



    for epoch in range(epochs):
        model.train()

        running_loss = 0
        epoch_rounds+=1
        print('â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”Start {}th rounds of trainingâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”'.format(epoch_rounds))

        for i in dataloader_train:          #è®­ç»ƒæ¨¡å‹
            graph, node_feats, edge_feats, protein_feats, labels ,adj_matrix= i  # è§£åŒ… tuple
            graph = graph.to(device)
            protein_feats = protein_feats.to(device)
            node_feats = node_feats.to(device)
            edge_feats = edge_feats.to(device)
            label = labels.to(device)
            label = label.view(-1,1)
            adj_matrix = adj_matrix.to(device)

            output = model(graph,node_feats,edge_feats,protein_feats,adj_matrix)

            loss = loss_trans(output, label)
            optim.zero_grad()  # æ¸…é™¤å…ˆå‰æ¢¯åº¦å‚æ•°ä¸º0

            loss.backward()  # åå‘ä¼ æ’­è®¡ç®— æŸå¤±å¯¹æ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„æ¢¯åº¦
            optim.step()

            running_loss += loss.item()
            train_round+=1
            if train_round % 1000==0:
                time_end = time.time()
                print('Training time:{}s'.format(time_end-time_start))
                print('The cross entropy loss for the {}th training is:{:.4f}'.format(train_round,loss))
            writer.add_scalar('realtime_Loss/Train_{}'.format(trial_id), loss.item(), global_step=train_round)       #è¾“å‡ºæ¯æ¬¡è®­ç»ƒçš„æŸå¤±å›¾åƒ

        avg_train_loss = running_loss / len(dataloader_train)
        print('The average loss of the {}th round of training is {:.4f}'.format(epoch+1,avg_train_loss))

        valid_loss_total=0
        acc_total=0
        labelscore_predicted = []
        labels_true = []
        model.eval()

        with torch.no_grad():
            for i in dataloader_valid:
                graph, node_feats, edge_feats, protein_feats, labels ,adj_matrix= i  # è§£åŒ… tuple
                graph = graph.to(device)
                protein_feats = protein_feats.to(device)
                node_feats = node_feats.to(device)
                edge_feats = edge_feats.to(device)
                label = labels.to(device)
                label = label.view(-1,1)      # æ•°æ®å‡†å¤‡
                adj_matrix = adj_matrix.to(device)  # æ•°æ®å‡†å¤‡

                output = model(graph, node_feats, edge_feats, protein_feats, adj_matrix)
                loss = loss_trans(output, label)
                valid_loss_total += loss.item()  * label.size(0)   #éªŒè¯é›†æ€»æŸå¤±

                label_probs = torch.sigmoid(output)  # å°† logits è½¬ä¸ºæ¦‚ç‡
                predicted = (label_probs > 0.5).float()      #æ˜¯å¦æœ‰æ´»æ€§
                acc = (predicted == label).sum().item()         #éªŒè¯é›†æ˜¯å¦æ­£ç¡®
                acc_total += acc        #éªŒè¯é›†æ€»æ­£ç¡®ä¸ªæ•°
                labels_true.extend(label.cpu().numpy().flatten())  # ROC
                labelscore_predicted.extend(label_probs.cpu().numpy().flatten())

        avg_valid_loss = valid_loss_total / len(dataloader_valid.dataset)
        avg_valid_acc = acc_total / len(dataloader_valid.dataset)

        auc = ROC_AUC(labelscore_predicted, labels_true,num=trial_number)
        print('The average accuracy on the validation set is:{:.4f}'.format(avg_valid_acc))
        print('Total loss on the validation set:{:.4f}'.format(valid_loss_total))
        print('Average loss on the validation set:{:.4f}'.format(avg_valid_loss))
        print('AUC on the validation set:{:.4f}'.format(auc))

        writer.add_scalar('avg_Loss/Train_{}'.format(trial_id), avg_train_loss, global_step=epoch)
        writer.add_scalar('avg_Loss/Valid_{}'.format(trial_id), avg_valid_loss, global_step=epoch)
        writer.add_scalar('Accuracy/Valid_{}'.format(trial_id), avg_valid_acc, global_step=epoch)

        # Early stopping check
        stop_flag, best_valid_loss, epochs_without_improvement, best_model_state, best_valid_acc, = (
            early_stopping(avg_valid_loss,
                           avg_valid_acc,
                           best_valid_loss,
                           model,
                           epoch,
                           epochs_without_improvement,
                           best_model_state=best_model_state,
                           best_valid_acc=best_valid_acc
                           ))
        if stop_flag == 'end':
            break

    # æ¢å¤æœ€å¥½çš„æ¨¡å‹
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        model.eval()

        save_dir = "ablation_model_pth"
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        if trial_id is not None:
            torch.save(model.state_dict(), os.path.join(save_dir, "best_model{}.pth".format(trial_id)))
        else:
            torch.save(model.state_dict(), "ablation_model_pth/best_model.pth")

        print('========================================================')
        print('Average loss on the validation set for the best model:{:.5f}'.format(best_valid_loss))
        print('The accuracy of the best model:{:.4f}'.format(best_valid_acc))
        print("The best model has been saved as best_model.pth")


    return best_valid_loss, best_valid_acc


def Bayesian(loss_trans,
             trials,
             device,
             dataset_train,
             dataset_valid
             ):
    writer_by_path = 'log_by'
    writer_by = SummaryWriter(writer_by_path)
    def objective(trial):       # è´å¶æ–¯ä¼˜åŒ–ç›®æ ‡å‡½æ•°
        trial_num = trial.number
        writer_path='log_by/log_by_{}'.format(trial_num)
        writer = SummaryWriter(writer_path)
        mpnn = MPNNPredictorWithProtein().to(device)  # ğŸ¯ é‡æ–°åˆå§‹åŒ–æ¨¡å‹

        mpnn.load_state_dict(torch.load('best_model0.pth', weights_only=False))  # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡

        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3,log=False)
        weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2,log=False)
        batch_size = trial.suggest_categorical('batch_size', [64,128])
        epochs = trial.suggest_int('epochs', 50, 100)

        best_valid_loss, best_valid_acc = train_valid(
            model=mpnn,
            loss_trans=loss_trans,
            learning_rate=learning_rate,
            weight_decay=weight_decay,
            batch_size=batch_size,
            epochs=epochs,
            device=device,
            dataset_train=dataset_train,
            dataset_valid=dataset_valid,
            trial_id=trial.number,
            writer=writer,
            trial_number=trial_num
        )
        trial.set_user_attr("valid_acc", float(best_valid_acc))

        writer_by.add_scalar('best_Loss/Valid', best_valid_loss, global_step=trial_num)
        writer_by.add_scalar('best_Accuracy/Valid', best_valid_acc, global_step=trial_num)
        writer_by.add_scalar('best_Accuracy/lr', best_valid_acc,
                             global_step=trial.params['learning_rate'])
        writer_by.add_scalar('best_Accuracy/wd', best_valid_acc,
                             global_step=trial.params['weight_decay'])

        return  best_valid_acc

    # study = optuna.create_study(direction='minimize')  # æœ€å°åŒ–éªŒè¯æŸå¤±
    study = optuna.create_study(direction='maximize',
                                pruner=optuna.pruners.HyperbandPruner(min_resource=10),
                                sampler=optuna.samplers.TPESampler())  # æœ€å¤§åŒ–æ­£ç¡®ç‡
    study.optimize(objective, n_trials=trials)  # ä¼˜åŒ–20æ¬¡è¯•éªŒ

    print('Best trial:')
    best_trial = study.best_trial
    print(f'  Trail: {best_trial.number}')
    print(f'  Average Accuracy: {best_trial.user_attrs["valid_acc"]:.4f}')
    print(f'  Loss Value: {best_trial.value}')
    print(f'  Params: {best_trial.params}')


def test(model,
         dataset_test,
         batch_size,
         loss_trans,
         device,
         Threshold,
         seed=None
         ):
    if seed is not None:
        set_seed()
    else:
        set_seed(seed)

    writer = SummaryWriter('test_logs')

    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, collate_fn=MolecularDataset.collate_fn,
                                 num_workers=0, pin_memory=False)
    valid_loss_total = 0
    acc_total = 0
    labels_true = []
    labelscore_predicted = []
    counter = 0

    # <<< æ–°åŠ ï¼šä¿å­˜é¢„æµ‹çš„æ ‡ç­¾
    labels_pred_binary = []

    # model.eval()
    with torch.no_grad():
        for i in dataloader_test:
            graph, node_feats, edge_feats, protein_feats, labels ,adj_matrix= i  # è§£åŒ… tuple
            graph = graph.to(device)
            protein_feats = protein_feats.to(device)
            node_feats = node_feats.to(device)
            edge_feats = edge_feats.to(device)
            label = labels.to(device)
            label = label.view(-1, 1)
            adj_matrix = adj_matrix.to(device)      # æ•°æ®å‡†å¤‡

            output = model(graph, node_feats, edge_feats, protein_feats,adj_matrix)
            loss = loss_trans(output, label)
            valid_loss_total += loss.item() * label.size(0)  # æµ‹è¯•é›†é›†æ€»æŸå¤±

            label_probs = torch.sigmoid(output)
            predicted = (label_probs > Threshold).float()  # æ˜¯å¦æœ‰æ´»æ€§

            acc = (predicted == label).sum().item()  # æµ‹è¯•é›†æ˜¯å¦æ­£ç¡®
            acc_total += acc  # éªŒè¯é›†æ€»æ­£ç¡®ä¸ªæ•°

            labels_true.extend(label.cpu().numpy().flatten())  # ROC
            labelscore_predicted.extend(label_probs.cpu().numpy().flatten())

            counter += 1

            # <<< æ–°åŠ ï¼šæ”¶é›†é¢„æµ‹æ ‡ç­¾ç”¨äºè®¡ç®—f1/mcc
            labels_pred_binary.extend(predicted.cpu().numpy().flatten())

            if counter % 100 == 0:
                print('å·²è®¡ç®—{}ç»„æ•°æ®'.format(counter))



    tn, fp, fn, tp = confusion_matrix(labels_true, labels_pred_binary).ravel()
    # <<< æ–°åŠ ï¼šæœ€ç»ˆç»Ÿè®¡f1åˆ†æ•°å’Œmcc
    precision = precision_score(labels_true, labels_pred_binary)
    recall = recall_score(labels_true, labels_pred_binary)
    specificity = tn / (tn + fp)
    f1 = f1_score(labels_true, labels_pred_binary)
    acc=acc_total / len(dataloader_test.dataset)
    mcc = matthews_corrcoef(labels_true, labels_pred_binary)
    auc = ROC_AUC(labelscore_predicted, labels_true)
    print('æµ‹è¯•é›†ä¸Šçš„æ­£ç¡®ç‡ï¼š{:.4f}'.format(acc))
    print('æµ‹è¯•é›†ä¸Šçš„æ€»æŸå¤±ï¼š{:.4f}'.format(valid_loss_total))
    print('æµ‹è¯•é›†F1åˆ†æ•°:{:.4f}'.format(f1))
    print('æµ‹è¯•é›†MCCåˆ†æ•°:{:.4f}'.format(mcc))
    print("æµ‹è¯•é›†ä¸Šçš„ ROC-AUC: {:.4f}".format(auc))
    print('æµ‹è¯•é›†ä¸Šçš„recall:{:.4f}'.format(recall))
    print('æµ‹è¯•é›†ä¸Šçš„precision:{:.4f}'.format(precision))
    print('æµ‹è¯•é›†ä¸Šçš„Specificity:{:.4f}'.format(specificity))

    writer.add_scalar('test_loss_total', valid_loss_total)  # è¾“å‡ºæ¯æ¬¡æµ‹è¯•çš„æ€»æŸå¤±å›¾
    writer.add_scalar('test_acc', acc_total / len(dataloader_test.dataset))  # è¾“å‡ºæ¯æ¬¡æµ‹è¯•çš„æ­£ç¡®ç‡

    return auc,acc,mcc,f1


def ROC_AUC( labelscore_predicted,labels_true,num=0):
    save_dir = "auc"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # è®¡ç®— FPR, TPR å’Œ é˜ˆå€¼
    fpr, tpr, thresholds = roc_curve(labels_true, labelscore_predicted)
    roc_auc = auc(fpr, tpr)  # è®¡ç®— AUC å€¼

    # ç»˜åˆ¶ ROC æ›²çº¿
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=1.5, label=f'ROC curve (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')  # éšæœºåˆ†ç±»å™¨å‚è€ƒçº¿
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (FPR)')
    plt.ylabel('True Positive Rate (TPR)')
    plt.title('ROC Curve')
    plt.legend(loc='lower right')
    plt.savefig('auc/roc_curve{}.png'.format(num), dpi=500)
    plt.close()
    return roc_auc

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print('using device:', device)

if __name__ == '__main__':
    while True:
        code= input('mission:')
        # code='bayesian'
        if code == 'train':
            writer_path = 'ablation_log_train'
            writer = SummaryWriter(writer_path)

            loss_trans = BCEWithLogitsLoss().to(device)
            mpnn = MPNNPredictorWithProtein().to(device)
            train_file_path = 'trainset.csv'
            valid_file_path = 'validset.csv'
            dataset_train = MolecularDataset.loading_data(train_file_path)
            dataset_valid = MolecularDataset.loading_data(valid_file_path)

            train_valid(model=mpnn,
                    loss_trans=loss_trans,
                    device=device,
                    learning_rate=0.0007451954932192549,
                    weight_decay=0.005574660133633517,
                    batch_size=64,
                    epochs=20,
                    dataset_train=dataset_train,
                    dataset_valid=dataset_valid,
                    writer=writer
                    )
            break

        elif code == 'bayesian':

            loss_trans = BCEWithLogitsLoss().to(device)

            train_file_path = 'train.csv'
            valid_file_path = 'val.csv'

            dataset_train = MolecularDataset.loading_data(train_file_path, device=device)
            dataset_valid = MolecularDataset.loading_data(valid_file_path, device=device)
            Bayesian(
                loss_trans=loss_trans,
                device=device,
                trials=50,
                dataset_train=dataset_train,
                dataset_valid=dataset_valid,
                )
            break

        elif code == 'test':
            loss_trans = BCEWithLogitsLoss().to(device)

            mpnn = MPNNPredictorWithProtein().to(device)
            mpnn.load_state_dict(torch.load('best_model.pth',weights_only=False) ) # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
            mpnn.eval()

            test_file_path = 'testset.csv'
            dataset_test = MolecularDataset.loading_data(test_file_path)
            test(model=mpnn,
                dataset_test=dataset_test,
                batch_size=64,
                loss_trans=loss_trans,
                device=device,
                Threshold=0.5,
                seed=4
                )
            break

        else:
            print('code erro')




